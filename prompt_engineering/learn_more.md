Attention Mechanism: RoPE (Rotary Position Embedding) with 128k context length
MOE ? 
MoE Design: Token-choice Mixture-of-Experts with SwiGLU activations for improved performance

builtin tools, pass their names in a list to the builtin_tools argument of the chat template, as shown below

ONNX Runtime,
Competition coding (Codeforces), 
general problem solving (MMLU and HLE) and 
tool calling (Tau-Bench agentic evaluation suite)
(HealthBench‚Å†) 
 competition mathematics (AIME 2024 & 2025)